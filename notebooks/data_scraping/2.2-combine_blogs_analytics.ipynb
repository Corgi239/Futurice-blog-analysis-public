{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read blog data and analytics data\n",
    "blogs = pd.read_csv('../data/blog_text.csv', \n",
    "    sep=',', \n",
    "    engine='python', \n",
    "    parse_dates=['time'], \n",
    "    date_parser=lambda col: pd.to_datetime(col, utc=True)\n",
    ")\n",
    "analytics = pd.read_csv('../data/google_analytics.csv')\n",
    "\n",
    "# Clean up category titles\n",
    "blogs['category'] = blogs['category'].str.replace('&amp;', 'and')\n",
    "\n",
    "# Convert time strings to datetime format\n",
    "blogs['time'] = blogs['time'].dt.date\n",
    "\n",
    "blogs['url'] = 'blog' + blogs['url'].str.split('blog', expand=True)[1].str.rstrip('/')\n",
    "\n",
    "# Combine the two tables\n",
    "combined = pd.merge(blogs, analytics, how='inner', on='url')\n",
    "\n",
    "# Drop entires that do not have urls OR pageviews\n",
    "combined = combined.dropna(subset=['url', 'pageviews'])\n",
    "\n",
    "# Infer titles from urls for entries that are missing a title\n",
    "#   1. Select entries that do not have a title\n",
    "#   2. For each selected entry take the URL\n",
    "#   3. Drop the first 5 character ('blog/')\n",
    "#   4. Replace dashes ('-') with whitespaces\n",
    "#   5. Use the altered URL as a new title for the entry\n",
    "mask = combined['title'].isna()\n",
    "combined.loc[mask, 'title'] = combined['url'][mask].str[5:].str.replace('-', ' ').str.capitalize()\n",
    "\n",
    "# Test\n",
    "combined.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of causes of NA values\n",
    "\n",
    "# First, let's take a look at rows with no text data\n",
    "combined[combined['text'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these URLs redirect to another page, so we can safely drop them without losing any valuable data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.dropna(axis=0, subset=['text'], inplace=True)\n",
    "combined.reset_index(inplace=True)\n",
    "combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's look at rows with no category\n",
    "\n",
    "combined[combined['category'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these articles once again redirect to different pages, but others seem like just normal blog posts. Perhaps the categories for those can be scraped when we move fully to BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the resulting merged table to file\n",
    "mask = combined['text'].str.contains(\"\\t\")\n",
    "combined.to_csv(\"../data/blogs_with_analytics.csv\", sep='\\t', index=False)\n",
    "\n",
    "# Test\n",
    "new_combined = pd.read_csv(\"../data/blogs_with_analytics.csv\", sep='\\t')\n",
    "combined.compare(new_combined)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
