{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapes all blog links form the \"page_num\":th page in the \"https://futurice.com/blog?page=\" and returns the urls found as a list of strings (or a empyt list)\n",
    "def scrape_one_base_page_for_urls(page_num):\n",
    "    urls = []\n",
    "    base_url = \"https://futurice.com/blog?page=\"\n",
    "    \n",
    "    r = requests.get(base_url + str(page_num))\n",
    "\n",
    "    # Check if was able to access the internet page\n",
    "    if r.status_code//100 != 2:\n",
    "        print(\"ERROR WHILE READING WEBPAGE\")\n",
    "        return List()\n",
    "    \n",
    "    # Parse the text\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    # Add all the links returned by the bs4 to the list\n",
    "    for item in soup.body.main.find_all(\"a\"):\n",
    "        if item.get(\"href\"):\n",
    "            urls.append(item.get(\"href\"))\n",
    "        \n",
    "    return urls\n",
    "\n",
    "\n",
    "# Scrapes all urls form all the pages (base page \"futurice.com/blog\") \n",
    "def scrape_all_main_pages_subpages(verbose = False):\n",
    "    scrape_more = True # A flag, that is turned false, when detected that the index:th page is last page\n",
    "    index = 1          # iteration index\n",
    "    url_count= 0\n",
    "    urls = []\n",
    "\n",
    "    while scrape_more:\n",
    "\n",
    "        urls.extend(scrape_one_base_page_for_urls(index))\n",
    "\n",
    "        # Check the flag conditions\n",
    "        if len(urls) == url_count:\n",
    "            scrape_more = False\n",
    "\n",
    "        if verbose: {print(str(index) + \": \" + str(len(urls)))}\n",
    "\n",
    "        index += 1\n",
    "        url_count = len(urls)\n",
    "\n",
    "\n",
    "    if verbose: {print(\"Finnished\")}\n",
    "    return urls\n",
    "\n",
    "\n",
    "# Helper function that writes the urls to file\n",
    "def write_to_file(urls, file_path = \"./urls\"):\n",
    "    with open(file_path, \"w\", encoding='utf-8') as file:\n",
    "        file.write(\"url\\n\")\n",
    "        for item in urls:\n",
    "            file.write(item + \"\\n\")\n",
    "\n",
    "\n",
    "def fetch_all_blog_urls_to_local():\n",
    "    urls = scrape_all_main_pages_subpages() \n",
    "    write_to_file(urls)\n",
    "\n",
    "\n",
    "# Reads all the urls from futurices main blog page (futurice.com/blog) and checks those against databases \"incompatible_blogs.csv\" \n",
    "# and \"../data/blogs_with_analytics.csvlogs_with_an\" files to return three values containing:\n",
    "#       new_blogs                           list of strings, containing all the paths to newly added blogs post, that this database hasn't seen\n",
    "#       removed_old_blogs                   list of strings, containing all the paths to blog posts, that have been added to this database, but that have been removed from the futurices page\n",
    "#       removed_old_incompatible_blogs      list of strings, containing all the paths to blog posts, that were tried to add to the database, but the blog was incompatible, and now have been removed from futurices webpage\n",
    "#\n",
    "# All the paths to blog post are of form \"blog/[blogs_own_unique_path]\". This is slower implementation, that checks all the pages.\n",
    "def get_blog_url_statuses():\n",
    "    # Blog urls that are currently in the database\n",
    "    df = pd.read_csv(\"../data/blogs_with_analytics.csv\", sep=\"\\t\", parse_dates=[\"time\"], infer_datetime_format=True, index_col=[\"index\"])\n",
    "    saved_urls = df[\"url\"].values\n",
    "\n",
    "    # Blog urls that have been tried, but were incompatible and therefore not added to database\n",
    "    incompatible_blogs = pd.read_csv(\"./incompatible_blogs.csv\", index_col=0)\n",
    "    incompatible_urls = [re.sub(\"\\S*futurice.com/\", \"\", word) for word in incompatible_blogs[\"urls\"]]\n",
    "\n",
    "    # Blog urls currently in the futurice webpages\n",
    "    urls = [word[1:] for word in scrape_all_main_pages_subpages()]\n",
    "\n",
    "    new_blogs = set(urls) - set(saved_urls) - set(incompatible_urls)\n",
    "    removed_old_blogs = set(saved_urls) - set(urls)\n",
    "    removed_old_incompatible_blogs = set(incompatible_blogs) - set(urls)\n",
    "\n",
    "    return new_blogs, removed_old_blogs, removed_old_incompatible_blogs\n",
    "\n",
    "\n",
    "# Returns the most recent newly added blogposts.\n",
    "# Faster implementation than \"get_blog_url_statuses\", but not as robust. \n",
    "def get_most_recent_newly_added_blogs():\n",
    "    scrape_more = True # A flag\n",
    "    index = 1          # iteration index\n",
    "    ret_urls = []      # urls, of new blogs (that are to be returned from the function call)\n",
    "\n",
    "    # Blog urls that are currently in the database\n",
    "    df = pd.read_csv(\"../data/blogs_with_analytics.csv\", sep=\"\\t\", parse_dates=[\"time\"], infer_datetime_format=True, index_col=[\"index\"])\n",
    "    saved_urls = df[\"url\"].values\n",
    "\n",
    "    while scrape_more:\n",
    "\n",
    "        temp_urls = scrape_one_base_page_for_urls(index)  #Temporally saved urls, that were fetched from the futurices index:th blog base page\n",
    "\n",
    "        if len(temp_urls) == 0:\n",
    "            scrape_more = False\n",
    "            break\n",
    "\n",
    "\n",
    "        for url in temp_urls:\n",
    "            if url[1:] in saved_urls:\n",
    "                scrape_more = False\n",
    "                break\n",
    "            ret_urls.append(url[1:])\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    return ret_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything after this is debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_recent_newly_added_blogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new, old, removed = get_blog_url_statuses()\n",
    "new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
