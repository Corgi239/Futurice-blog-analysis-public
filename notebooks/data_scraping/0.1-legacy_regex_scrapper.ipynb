{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Some global constants\n",
    "url_base = \"https://futurice.com/\"\n",
    "file_base = str(os.path.dirname(os.path.dirname(os.path.abspath(\"regex_scrapper_legacy\")))) + \"/data/\"\n",
    "sep = ';;;'  # separator for csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the raw text from url and return it as utf-8 string\n",
    "def download_raw(url):\n",
    "    headers = {\"User-Agent\" : \"Mozilla/5.0 (Windows NT 5.1; rv:7.0.1) Gecko/20100101 Firefox/7.0.1\"} \n",
    "    r_temp  = requests.get(url_base + url)#, headers)\n",
    "\n",
    "    string = bytearray()\n",
    "\n",
    "    for chunk in r_temp.iter_content(chunk_size=128):\n",
    "        string.extend(chunk)\n",
    "\n",
    "    return string.decode(encoding='utf-8')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapes the text from input string and parses most of the unnecessary characters and marking out\n",
    "def process_blog_post(string):\n",
    "    \n",
    "    texts = \"\"\n",
    "\n",
    "    for item in re.findall('\"text\":\".*?\"}', string):\n",
    "        \n",
    "        text = item[len('\"text\":\"'):-1]\n",
    "        #print(text)\n",
    "\n",
    "        # Idunno why this works but removing only the \\n ones doesn't\n",
    "        text = re.sub('\\\\\\\\n',  \" \", text)\n",
    "        text = re.sub('\\\\\\n',  \" \", text)\n",
    "        text = re.sub('\\\\n',  \"\", text)\n",
    "        text = re.sub('\\n',  \"\", text)\n",
    "        text = re.sub(\"\\'s\", \"'s\", text)\n",
    "        text = re.sub(\"\\'\", \"\", text)\n",
    "        text = re.sub(\"#\", \"\", text)\n",
    "\n",
    "        # Removing unnecessary unicodes?\n",
    "        #text = re.sub(\"\\\\u.*? \", \" \", text)\n",
    "\n",
    "        text = re.sub(\"\\((http.*?|s://)\\)\", \"\", text)\n",
    "        text = re.sub(\"(s://.*?)\", \"\", text)\n",
    "        text = re.sub(\"!\\[.*?\\]\\(.*?\\)\", \"\", text)\n",
    "\n",
    "\n",
    "        text = re.sub(\"\\[\", \"\", text)\n",
    "        text = re.sub(\"\\]\", \"\", text)\n",
    "        \n",
    "        texts += text\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_links(string):\n",
    "    \n",
    "    ret = []\n",
    "    for i in re.findall(r'blog/[a-zA-Z0-9/-]+[a-zA-Z0-9]+', string):\n",
    "        if i not in ret:\n",
    "            ret.append(i)\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(string): \n",
    "    #print(\"title: \" + str(re.search('\"title\":\".*?\"', string).group()[len('\"title\":\"'):-1]))\n",
    "    temp = re.search('</title>.*?content=\".*?\"', string)\n",
    "    if temp == None:\n",
    "        print(\"----Error: title not found\")\n",
    "        return \"\"\n",
    "    else:\n",
    "        return temp.group()[len(re.search('</title>.*?content=\"', string).group()):-1]\n",
    "    \n",
    "def get_teaser(string):\n",
    "    #print(\"teaser: \" + str(re.search('\"teaser\":\".*?\"', string).group()[len('\"teaser\":\"'):-1]))\n",
    "    temp = re.search('\"teaser\":\".*?\"', string)\n",
    "    if temp == None:\n",
    "        print(\"----Error: teaser not found\")\n",
    "        return \"\"\n",
    "    else:\n",
    "        return temp.group()[len('\"teaser\":\"'):-1]\n",
    "\n",
    "def get_datetime(string):\n",
    "    temp = re.search('<time dateTime=\".*?\"', string)\n",
    "    if temp == None:\n",
    "        print(\"----Error: date not found\")\n",
    "        return \"\"\n",
    "    else:\n",
    "        return temp.group()[len('<time dateTime=\"'):-1]\n",
    "\n",
    "def get_category(string):\n",
    "    temp = re.search('<span class=\"sc-7f8efa2d-1 gwaujb\">.*?</span>', string)\n",
    "    if temp == None:\n",
    "        print(\"----Error: category not found\")\n",
    "        return \"\"\n",
    "    else:\n",
    "        return temp.group()[len('<span class=\"sc-7f8efa2d-1 gwaujb\">'):-len('</span>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes string and exports it to file. If csv = True, appends it as csv row to file_name, otherwise creates\n",
    "# a new json file. Returns a dictonary with found values.\n",
    "def process_file_to_data(string, url, csv = False, file_name = \"blog_text.csv\"):\n",
    "    \n",
    "    title = get_title(string)\n",
    "    teaser = get_teaser(string)\n",
    "    time  = get_datetime(string)\n",
    "    category = get_category(string)\n",
    "    text = process_blog_post(string)\n",
    "    \n",
    "        \n",
    "    if not csv:\n",
    "        with open(file_base + url, \"w\", encoding='utf-8') as fd:\n",
    "            json.dump({\n",
    "                \"url\" : url,\n",
    "                \"title\" :title,\n",
    "                \"time\" : time,\n",
    "                \"category\" : category,\n",
    "                \"text\" : text\n",
    "            }, fd, indent = 4)\n",
    "    else:\n",
    "        with open(file_base + file_name, \"a\", encoding='utf-8') as fd:\n",
    "            fd.write(url + sep + title + sep + time + sep + category + sep + text + \"\\n\")\n",
    "    \n",
    "    return {    \"url\" : url,\n",
    "                \"title\" :title,\n",
    "                \"time\" : time,\n",
    "                \"category\" : category,\n",
    "                \"text\" : text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_urls():\n",
    "    df = pd.read_csv(file_base + \"blog_urls_all.csv\", encoding = 'utf-8')\n",
    "    df = df.iloc[:,0].str.replace('^/', '').tolist()\n",
    "    return df\n",
    "        \n",
    "get_base_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_loading(max_num = 900, to_csv = True, create_new = True):\n",
    "\n",
    "    unread_urls = get_base_urls()\n",
    "    linked = dict()\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    if to_csv and create_new:\n",
    "        # Writing labels to file\n",
    "        file_name = \"blog_text.csv\"\n",
    "        with open(file_base + file_name, \"w\", encoding='utf-8') as fd:\n",
    "            fd.write(\"url\" + sep + \"title\" + sep + \"time\"+ sep  + \"category\"+ sep  + \"text\" + \"\\n\")\n",
    "\n",
    "    #Looping through urls until max_num is reached or there is nothing in the unread_urls\n",
    "    while len(unread_urls) > 0 and count < max_num:\n",
    "        count += 1\n",
    "        url = unread_urls.pop(0)\n",
    "        print(str(count) + \": \" + url)\n",
    "\n",
    "        string = download_raw(url)\n",
    "        temp_dict = process_file_to_data(string, url, csv = to_csv)\n",
    "\n",
    "        temp_array = []\n",
    "        for item in get_blog_links(string):\n",
    "            temp_array.append(item)\n",
    "        linked[url] = temp_array\n",
    "\n",
    "    if not to_csv:\n",
    "        # Writing a base file with information of all the url/names of the blog text files        \n",
    "        with open(file_base + \"urls.csv\", \"w\", encoding='utf-8') as fd:\n",
    "            for item in read_urls:\n",
    "                fd.write(item)\n",
    "                fd.write(\"\\n\")\n",
    "\n",
    "    # saves the urls of the unread ones, if one doesn't want to run all the samples in one go        \n",
    "    if len(unread_urls) > 0:\n",
    "        with open(file_base + \"unread.csv\", \"w\", encoding='utf-8') as fd:\n",
    "            for item in unread_urls:\n",
    "                fd.write(item)\n",
    "                fd.write(\"\\n\")\n",
    "\n",
    "    # Saving to json file, which blog post linked to which other      \n",
    "    if len(linked) > 0:\n",
    "        with open(file_base + \"linked.json\", \"w\", encoding='utf-8') as fd:\n",
    "            json.dump(linked, fd, indent = 4)\n",
    "\n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_base + \"blog_text.csv\", sep = sep, engine='python', encoding = 'utf-8', names = [\"url\", \"title\", \"timedate\", \"category\", \"text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
