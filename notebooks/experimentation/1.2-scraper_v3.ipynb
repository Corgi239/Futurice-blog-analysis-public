{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper (that finished a bit later than ideal)\n",
    "\n",
    "### Table of contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Importing Libraries](#imports)\n",
    "    1. [Starting Selenium Webdriver](#selenium)\n",
    "3. [Reading in URLs from CSV](#reading)\n",
    "4. [Scraping](#scraping)\n",
    "5. [Exporting](#writing)\n",
    "6. [Appendix](#appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "This webscraper has been tailored for Futurice Blogs (https://futurice.com/blog) but with some analysis of blog structure and tweaking, can be adapted to other websites as well.\n",
    "\n",
    "Here, we collect relevant features of blogs along with their main body for future NLP analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"selenium\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting browser to Firefox and initialising as headless (without the browser opening up in a window)\n",
    "# NOTE: Requires you to install Firefox's Geckodriver onto your system, see online documentation.\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "options = FirefoxOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV\n",
    "urls_list = pd.read_csv(\"FullUrls.csv\")\n",
    "urls_list = list(urls_list[\"urls\"])\n",
    "\n",
    "# Slice the list into 50 url chunks\n",
    "chunks = [urls_list[x:x+100] for x in range(0, len(urls_list), 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scraping\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of urls that aren't compatible with scraper\n",
    "list_of_incompatible_blogs = []\n",
    "\n",
    "# Initialise DataFrame with column names\n",
    "df = pd.DataFrame(columns=[\"url\",\"title\",\"date\",\"category\",\"description\",\"body\",\"introduction\",\"author\",\"author_job_title\"])\n",
    "\n",
    "\n",
    "# Start iterating through the blogs\n",
    "for chunk in chunks:\n",
    "\n",
    "    # Initialise a list which will contain our blog objects (dictionaries)\n",
    "    list_of_blog_objects = []\n",
    "\n",
    "\n",
    "    #iterate through the urls in each chunk\n",
    "    for url in chunk:\n",
    "\n",
    "        # Get the blog\n",
    "        driver.get(url)\n",
    "\n",
    "        # Title of the blog\n",
    "        title = driver.title\n",
    "\n",
    "        # Date the blog was published. In Datetime format\n",
    "        try:\n",
    "            date = driver.find_element(By.CLASS_NAME, \"sc-adf7a739-0.dbnxnV\").get_attribute(\"datetime\")\n",
    "        except:\n",
    "            date = \"This blog is incompatible\"\n",
    "            list_of_incompatible_blogs.append(url)\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Category the blog is under\n",
    "        category = driver.find_element(By.CLASS_NAME, \"sc-adf7a739-1.lkSQky\").text\n",
    "\n",
    "        # Description of the blog for SEO, different from the blurb at the top of the blog\n",
    "        description = driver.find_element(By.XPATH, \"//meta[@name='description']\").get_attribute(\"content\")\n",
    "\n",
    "        # The main body of the blog\n",
    "        try:\n",
    "            body = driver.find_element(By.CLASS_NAME, \"sc-62bcb39b-0.kwENye.bodySection_body__wrapper__dyPBE\").text\n",
    "        except:\n",
    "            body = \"This blog is incompatible\"\n",
    "            list_of_incompatible_blogs.append(url)\n",
    "            continue\n",
    "\n",
    "        # The introduction blurb\n",
    "        try:\n",
    "            introduction = driver.find_element(By.CLASS_NAME, \"introduction_intro__text__wT0nc\").text\n",
    "        except:\n",
    "            # In case there is no introduction, we use the first 30 words of the body\n",
    "            try:\n",
    "                introduction = body.split()[:30]\n",
    "                result_str = \" \".join(body)\n",
    "            # In case there is no body\n",
    "            except:\n",
    "                introduction = \"This blog is incompatible\"\n",
    "\n",
    "        \n",
    "        # The authors of the blog and their job titles\n",
    "\n",
    "        # Getting a list of Selenium elements that match the class name,\n",
    "        # Mapping a function that converts those elements to strings\n",
    "        author = driver.find_elements(By.CLASS_NAME, \"sc-b0268d1f-5.hePoge\")\n",
    "        author = list(map(lambda x: x.text, author))\n",
    "        \n",
    "        # Same as above\n",
    "        author_job_title = driver.find_elements(By.CLASS_NAME, \"sc-b0268d1f-6.kOgHwO\")\n",
    "        author_job_title = list(map(lambda x: x.text, author_job_title))\n",
    "\n",
    "        # Checking for number of authors. \n",
    "        # If 1, assigns to variable\n",
    "        # If >1, concatenates with ':' in the middle and assigns to variable\n",
    "        # If 0, defaults to \"No ...\" \n",
    "        if (len(author) == 1):\n",
    "            author = author[0]\n",
    "            author_job_title = author_job_title[0]\n",
    "        elif (len(author) > 1):\n",
    "            author = ':'.join(author)\n",
    "            author_job_title = ':'.join(author_job_title)\n",
    "        else:\n",
    "            author = \"No Author\"\n",
    "            author_job_title = \"No Author Position\"\n",
    "        \n",
    "        # Empty dictionary representing each blog \n",
    "        blog_object = {}\n",
    "\n",
    "        # Assigning key-value pairs to the dictionary\n",
    "        blog_object[\"url\"] = url\n",
    "        blog_object[\"title\"] = title\n",
    "        blog_object[\"date\"] = date\n",
    "        blog_object[\"category\"] = category\n",
    "        blog_object[\"description\"] = description\n",
    "        blog_object[\"body\"] = body\n",
    "        blog_object[\"introduction\"] = introduction\n",
    "        blog_object[\"author\"] = author\n",
    "        blog_object[\"author_job_title\"] = author_job_title\n",
    "\n",
    "        # Appending blog object to list\n",
    "        list_of_blog_objects.append(blog_object)\n",
    "    \n",
    "\n",
    "    # Convert the list of blog objects into a DataFrame    \n",
    "    temporary_df_holder = pd.DataFrame(list_of_blog_objects)    \n",
    "\n",
    "    # Concatenate the latest DataFrame with existing DataFrame, ignore the index of the new one \n",
    "    # { Every temporary DataFrame will have index [1:50], if we don't specify ignore parameter, \n",
    "    # pandas will try to preserve old and new indices and throw an error }\n",
    "\n",
    "    df = pd.concat([df, temporary_df_holder], ignore_index=True)    \n",
    "    \n",
    "    # Free up memory held by both current chunk of blogs (in list) and temporary DataFrame \n",
    "    del list_of_blog_objects\n",
    "    del temporary_df_holder\n",
    "    \n",
    "    # Wait 15 seconds between chunks to not trip the bot detection tools\n",
    "    time.sleep(15)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"writing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as CSV\n",
    "df.to_csv(\"total_blog_data.csv\")\n",
    "\n",
    "# Save urls of incompatible blogs as CSV\n",
    "pd.DataFrame(list_of_incompatible_blogs, columns = [\"urls\"]).to_csv(\"incompatible_blogs.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix\n",
    "\n",
    "##### Appendix 1\n",
    "\n",
    "What I used to convert Laura's url tails to full urls\n",
    "\n",
    "Turn the urls csv into a DataFrame and give the column the name \"tails\"\n",
    "```python\n",
    "urls_list = pd.read_csv(\"blog_urls.csv\", names = [\"tails\"])\n",
    "```\n",
    "\n",
    "Use a lambda function to stitch the beginning of the urls to all the elements\n",
    "```python\n",
    "head = \"https://futurice.com\"\n",
    "assembler = lambda tail: head + tail\n",
    "urls_list = urls_list[\"tails\"].map(assembler)\n",
    "```\n",
    "\n",
    "Use the resulting list to make the final CSV of URLs \n",
    "```python\n",
    "urls_list.to_csv(\"FullUrls\")\n",
    "```\n",
    "##### Appendix 2\n",
    "\n",
    "To get the first 200 characters of the body as the introduction blurb instead of some number of words\n",
    "```python\n",
    "try:\n",
    "    introduction = driver.find_element(By.CLASS_NAME, \"introduction_intro__text__wT0nc\")\n",
    "except:\n",
    "    introduction = body[0:200]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
