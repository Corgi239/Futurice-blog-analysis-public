{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_ANALYTICS_START_DATE = datetime.date(2019,9,13)\n",
    "\n",
    "df = pd.read_csv(\n",
    "        '../../data/final/futurice_blog_data.csv', \n",
    "        sep='\\t', \n",
    "        parse_dates=['time'],\n",
    "        date_parser=lambda col: pd.to_datetime(col))\n",
    "df = df[df['time'] >= pd.Timestamp(GOOGLE_ANALYTICS_START_DATE)]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr_matrix = df.corr()\n",
    "performance_features = ['pageviews', 'avg_time', 'bounce_rate', 'exit%']\n",
    "predictors = sorted(list(set(df.select_dtypes(include=['float64', 'int64']).columns) - set(performance_features) - set(['index', 'unique_pageviews'])))\n",
    "corr_matrix = corr_matrix.loc[predictors, performance_features]\n",
    "corr_matrix.style.background_gradient(cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance with linear regression\n",
    "\n",
    "## Lasso coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "alpha_log_limits = [(-2,3),(-3,2),(-4,-1),(-4,-1)]\n",
    "lasso = Lasso(max_iter=10000)\n",
    "opt_models = []\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "for i, target_feature in enumerate(performance_features):\n",
    "    alphas = np.logspace(*alpha_log_limits[i], 1000)\n",
    "    X = df[predictors]\n",
    "    y = df[target_feature]\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "    # Normalize predictors\n",
    "    scaler = StandardScaler().fit(X_train) \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # Train a Lasso predictor for each alpha\n",
    "    coefs = []\n",
    "    for a in alphas:\n",
    "        lasso.set_params(alpha=a)\n",
    "        lasso.fit(X_train, y_train)\n",
    "        coefs.append(lasso.coef_)\n",
    "    # Train an optimal Lasso predictor using CV\n",
    "    model = LassoCV(cv=5, random_state=0, max_iter=10000, eps=1e-4)\n",
    "    model.fit(X_train, y_train)\n",
    "    opt_models.append(model)\n",
    "    opt_alpha = model.alpha_\n",
    "    # Plot the coefficients over alpha\n",
    "    ax = plt.subplot(1,4,i+1)\n",
    "    ax.plot(alphas, coefs, label=predictors)\n",
    "    ax.axvline(x=opt_alpha, color='r', ls=':', lw=0.5, label='alpha: CV estimation')\n",
    "    ax.set_xscale('log')\n",
    "    ax.axis('tight')\n",
    "    ax.set_xlabel('alpha')\n",
    "    ax.set_ylabel('Standardized Coefficients')\n",
    "    ax.set_title(f'Lasso coefficients for {target_feature}') \n",
    "plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "for i, target_feature in enumerate(performance_features):\n",
    "    X = df[predictors]\n",
    "    y = df[target_feature]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "    scaler = StandardScaler().fit(X_train) \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    model = opt_models[i]\n",
    "    top_features = [predictors[i] for i in np.argpartition(np.abs(model.coef_), -3)[-3:]]\n",
    "    ax = plt.subplot(2,2,i+1)\n",
    "    bar_colors = [3 if feat in top_features else 0 for feat in predictors]\n",
    "    ax.axvline(0, color='black', lw=0.7)\n",
    "    ax.barh(predictors, model.coef_, fc='royalblue', ec='firebrick', lw=bar_colors)\n",
    "    ax.set_title(f'Importance scores for predicting {target_feature}')\n",
    "    # ax.set_xticks(range(len(predictors)), predictors, rotation=45)\n",
    "    print(f'-- Evaluating optimal Lasso model for predicting [{target_feature}]')\n",
    "    print(' Top features: ', \", \".join(top_features))\n",
    "    print(' RMSE: ', math.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n",
    "    print()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear models don't do so well with predicting the performance of the blog, which indicates that the importance scores might also be unreliable. Let's try another approach. Instead of linear regression, we can use random tree forest regressors to asses importance of the features based on the reduction in the criterion used to select split points, like Gini or entropy.\n",
    "\n",
    "# Random Forest Regressor feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "for i, target_feature in enumerate(performance_features):\n",
    "    X = df[predictors]\n",
    "    y = df[target_feature]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    top_features = [predictors[i] for i in np.argpartition(model.feature_importances_, -3)[-3:]]\n",
    "    bar_colors = [3 if feat in top_features else 0 for feat in predictors]\n",
    "    ax = plt.subplot(2,2,i+1)\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "    ax.barh(predictors, np.abs(model.feature_importances_), xerr=std, fc='royalblue', ec='firebrick', lw=bar_colors)\n",
    "    ax.set_title(f'Importance scores for predicting {target_feature}')\n",
    "    ax.set_xlim(left=0)\n",
    "    print(f'-- Evaluating RFR model for predicting [{target_feature}]')\n",
    "    print(' Top features: ', \", \".join(top_features))\n",
    "    print(' RMSE: ', math.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n",
    "    print()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
