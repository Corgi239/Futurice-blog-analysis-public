{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytrends.request import TrendReq \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetches the missing keywords from google trends\n",
    "def get_google_trends_to_csv(keyword, df, df_index, df_name = \"../../data/interim/trend_single_score.csv\"):\n",
    "    comp_time = \"2010-01-01 2022-11-19\"\n",
    "    pytrends = TrendReq(hl='en-US', tz=-120, timeout=(10,25), retries = 4, backoff_factor=10)\n",
    "    pytrends.build_payload([keyword], timeframe=comp_time, geo = \"\")\n",
    "    loc_df = pytrends.interest_over_time()\n",
    "\n",
    "    if(len(loc_df) < 1):\n",
    "        loc_df = pytrends.interest_over_time()\n",
    "        if(len(loc_df) < 1):\n",
    "            with open(\"../../data/interim/lift_score_unpopular_google_searches.txt\", \"a\") as file:\n",
    "                file.write(str(df_index) + \": \" + keyword +\"\\n\")\n",
    "                return df, False\n",
    "\n",
    "    loc_df = loc_df.drop(columns=\"isPartial\")\n",
    "    df = pd.concat([df, loc_df], axis=1)\n",
    "\n",
    "    df.to_csv(df_name)\n",
    "\n",
    "    return df, True\n",
    "\n",
    "#Calculates the lift score = has the word been more or less trendy this month than on average within the last year\n",
    "def get_lift(keyword, df, year, month, df_index):\n",
    "    if keyword not in df.columns:\n",
    "        print(\"    adding \" + keyword + \" to data base.\")\n",
    "        df, succeeded = get_google_trends_to_csv(keyword, df, df_index)\n",
    "        if not succeeded:\n",
    "            print(\"    adding failed.\")\n",
    "            return df, 1\n",
    "\n",
    "    end_index = int(np.where((df.index.year ==  year) & (df.index.month == month))[0])\n",
    "    start_index = int(np.where((df.index.year ==  year-1) & (df.index.month == month))[0])\n",
    "\n",
    "    month_score = df.iloc[end_index][keyword]\n",
    "    mean = df.iloc[start_index:end_index][keyword].mean()\n",
    "\n",
    "    return df, month_score/(max(mean, 1))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximal Marginal Relevance\n",
    "# Returns top_n best keywords\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesses text and call mmr\n",
    "# Returns top_n keywords\n",
    "def get_mmr_keywords(doc, top_n=5):\n",
    "\n",
    "    n_gram_range = (1,1)\n",
    "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=\"english\").fit([doc])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "    doc_embedding = model.encode([doc])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    #top_n = 10\n",
    "    #distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    #keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "    #keywords\n",
    "    if len(candidates) <= top_n:\n",
    "        return candidates\n",
    "     \n",
    "    return mmr(doc_embedding, candidate_embeddings, candidates, top_n=top_n, diversity=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/interim/blogs_with_analytics.csv\", sep=\"\\t\", parse_dates=[\"time\"], infer_datetime_format=True)\n",
    "keyword_df = pd.read_csv(\"../../data/interim/trend_single_score.csv\", parse_dates=[\"date\"], infer_datetime_format=True, index_col=[\"date\"])\n",
    "# pytrends = TrendReq(hl='en-US', tz=-120, timeout=(10,25), retries = 4, backoff_factor=10)\n",
    "# keyword = \"Google\"\n",
    "# comp_time = \"2010-01-01 2022-11-19\"\n",
    "# pytrends.build_payload([keyword], timeframe=comp_time, geo = \"\")\n",
    "# loc_df = pytrends.interest_over_time()\n",
    "# loc_df = loc_df.drop(columns=\"isPartial\")\n",
    "# keyword_df = loc_df\n",
    "for index in range(1, len(df)):\n",
    "    keywords = get_mmr_keywords(df.iloc[index][\"text\"])\n",
    "    print(str(index) + \": \" + str(keywords))\n",
    "    \n",
    "    lift_sum = 0\n",
    "\n",
    "    for word in keywords: \n",
    "        keyword_df, lift = get_lift(word, keyword_df, int(df.iloc[index][\"time\"].year), int(df.iloc[index][\"time\"].month), index)\n",
    "        lift_sum += lift\n",
    "        df.at[index, \"mmr_lift\"] = lift_sum\n",
    "\n",
    "    df.to_csv(\"../../data/interim/blogs_with_analytics.csv\", sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rest of the file is for debuging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mmr_keywords(df.iloc[772][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[772]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mmr_lift\"][0:639].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_time = \"2010-01-01 2022-11-19\"\n",
    "pytrends = TrendReq(hl='en-US', tz=-120, timeout=(10,25), retries = 4, backoff_factor=10)\n",
    "pytrends.build_payload([\"mobprogramming\"], timeframe=comp_time, geo = \"\")\n",
    "loc_df = pytrends.interest_over_time()\n",
    "loc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"../data/blogs_with_analytics.csv\", sep=\"\\t\", parse_dates=[\"time\"], infer_datetime_format=True, index_col=[\"index\"])\n",
    "#temp_df = temp_df.drop(columns=temp_df.columns[0:1])\n",
    "#temp_df.to_csv(\"../data/blogs_with_analytics.csv\", sep=\"\\t\")\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df[\"mmr_lift\"] = temp_df[\"mmr_lift\"].fillna(-1.0)\n",
    "temp_df.info()\n",
    "temp_df.to_csv(\"../data/blogs_with_analytics_backup.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
