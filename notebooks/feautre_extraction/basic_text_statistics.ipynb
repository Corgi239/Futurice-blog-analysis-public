{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to calculate and add some basic text statistic to the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textstat\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from itertools import chain\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')    \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the data\n",
    "\n",
    "df = pd.read_csv(\"../../data/interim/blogs_with_analytics.csv\", sep=\"\\t\")\n",
    "df.dropna(subset=[\"text\"], inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average sentence length\n",
    "In this part, the average sentence length of each blog is calculated to form a new column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process to calculate the average length is as follow:\n",
    "\n",
    "-   The texts are tokenized using `nltk`'s `sent_tokenize` method\n",
    "-   The words that have the form a number followed by a dot , for example 1., 2. ,... will be removed from the sentences\n",
    "-   The sentences will be further cleaned, such that words of the form some punctuations followed by text will have the punctuations removed. For example '//u003e' will be converted into 'u003e'\n",
    "-   Finally, the average sentence length will be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### AVERAGE SENTENCE LENGTH IN THE TEXT\n",
    "texts = df[\"text\"].astype(str)\n",
    "sents_df = [sent_tokenize(sent) for sent in texts]\n",
    "\n",
    "sents_df = [ [re.sub(pattern=\"\\d+[.]\",repl=\"\", string=sent.strip()) for sent in sent_df] for sent_df in sents_df ]\n",
    "sents_df = [ [re.sub(pattern=\"[^a-zA-Z0-9\\s]\",repl=\"\", string=sent) for sent in sent_df] for sent_df in sents_df ]\n",
    "\n",
    "## Filter out the strings that only contains a white space\n",
    "res_df = [ [ sent.strip().replace('\\r', '.').replace('\\n', '.').split('.') for sent in sent_df if sent != \"\" ] for sent_df in sents_df ]\n",
    "res_df = [ [sentence.strip() for sentences in bunch for sentence in sentences if sentence != ''] for bunch in res_df ]\n",
    "\n",
    "splitted_df = [ [ [char for char in sent.split(\" \") if char != \"\"] for sent in res] for res in res_df ]\n",
    "avg_df = [ np.mean([len(chunk) for chunk in spliting]) for spliting in splitted_df ]\n",
    "\n",
    "\n",
    "# df[\"average_sentence_length\"] = np.array(avg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE:_** There are two problematics results:\n",
    "\n",
    "-   The blog with url 'blog/hacker-news-favorites' has some code for a table mixed in with the text, thus the average sentence length is over 300\n",
    "-   The blog with url 'blog/cycleconf-2017-attracted-some-very-different-cyclists-to-stockholm-this-spring' has the text only as .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total words in a text\n",
    "The calculation for this part is essentially the average but instead of using `mean` in the last part we use `sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df = [ np.sum([len(chunk) for chunk in spliting]) for spliting in splitted_df ]\n",
    "df['text_length'] = np.array(sum_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text readability\n",
    "\n",
    "This will consider some readability statistics:\n",
    "\n",
    "-   Dale-Chall readability formula (Using the new Dale-Chall formula)\n",
    "-   Flesh-Kincaid readability tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "dale_chall = np.full(df.shape[0], -1, float)\n",
    "flesch = np.full(df.shape[0], -1, float)\n",
    "\n",
    "for i, text in enumerate(df.text):\n",
    "    dale_chall[i] = textstat.dale_chall_readability_score(text)\n",
    "    flesch[i] = textstat.flesch_reading_ease(text)\n",
    "\n",
    "\n",
    "df[\"dale_chall\"] = dale_chall\n",
    "df[\"flesch\"] = flesch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average stopwords per sentence\n",
    "The code block below is to test for one text only, change the `run_test_stopword` flag to `True` to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"text\"].astype(str)\n",
    "sents_length_df = np.array([len(sent_tokenize(sent)) for sent in texts])\n",
    "\n",
    "stopwords_df = np.array([word_tokenize(text) for text in texts])\n",
    "stopwords_df = np.array([len([w for w in tokens if w in stopwords.words('english')]) for tokens in stopwords_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the arrays, we use the `divide` function of `numpy` to get the desired column. The reason I split this into two blocks is to avoid running the array generating code again if something needed to be change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"average_stopword\"] = np.divide(stopwords_df, sents_length_df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the generated statistic into the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"../data/blogs_with_analytics.csv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
