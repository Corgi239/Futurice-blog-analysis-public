{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf81135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18380664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/final/futurice_blog_data.csv', sep='\\t', engine='python')\n",
    "\n",
    "#Foresight and importance of foresight methods in business blog \n",
    "doc = df['text'].iloc[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "785eeed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# modify this to adjust the length of phrase, e.g.(2,2) for phrases of 2 words\n",
    "n_gram_range = (1,2)\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=\"english\").fit([doc])\n",
    "candidates = count.get_feature_names()\n",
    "#candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1655ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8cbb7980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['instagram',\n",
       " 'invention bitcoin',\n",
       " 'technology revolutionise',\n",
       " 'started cryptocurrency',\n",
       " 'cryptocurrency party',\n",
       " 'twitter instagram',\n",
       " 'driving hype',\n",
       " 'numerous podcasts',\n",
       " 'instagram social',\n",
       " 'cryptocurrency rapidly']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 10\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394416ae",
   "metadata": {},
   "source": [
    "## Diversification\n",
    "\n",
    "More diverse -> less likely to represent the document as a collective\n",
    "\n",
    "Find the balance between accuracy of keywords and their diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab3cb3",
   "metadata": {},
   "source": [
    "**Max Sum Similarity**\n",
    "\n",
    "Maximize candidate similarity with document while minimizing similarity between the candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d01c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf0644b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technology revolutionise',\n",
       " 'cryptocurrency party',\n",
       " 'twitter instagram',\n",
       " 'numerous podcasts',\n",
       " 'cryptocurrency rapidly']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have a more diverse set of keyphrases\n",
    "# low nr_candidates => less diverse\n",
    "# high nr_candidates => more diverse\n",
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d435f1",
   "metadata": {},
   "source": [
    "**Maximal Marginal Relevance**\n",
    "\n",
    "EmedRank used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93d388dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82c67be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cryptocurrency rapidly',\n",
       " 'instagram social',\n",
       " 'numerous podcasts',\n",
       " 'driving hype',\n",
       " 'cryptocurrency party']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914d76f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
