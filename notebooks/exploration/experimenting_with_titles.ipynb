{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRu9RziftcUX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth, KMeans, DBSCAN\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI31bbDXtcUY"
      },
      "outputs": [],
      "source": [
        "### Import the data\n",
        "\n",
        "df = pd.read_csv('../../data/final/futurice_blog_data.csv', delimiter='\\t')\n",
        "print(df.info())\n",
        "\n",
        "### Drop the rows that have NaN title:\n",
        "df.dropna(subset=['title'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq8X6QpqtcUZ"
      },
      "outputs": [],
      "source": [
        "data = np.array(df['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Use for tokenize in the tf-idf. Taken from http://brandonrose.org/clustering#Visualizing-document-clusters\n",
        "\n",
        "# Stemmer from nltk snowball stemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Using tfidf_Vectorizer to calculate the tfidf matrix\n",
        "# tfidf_vectorizer = TfidfVectorizer(max_df=0.5,\n",
        "                                #  min_df=5, stop_words='english',\n",
        "                                #  use_idf=True, tokenizer=tokenize_and_stem)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=5, stop_words=\"english\")\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Function for finding the best epsilon\n",
        "## The feature matrix should be in the form of row matrix, meaning that each sample is in one row of the matrix\n",
        "## k is the number of nearest point that the function will consider\n",
        "\n",
        "\n",
        "def best_eps_cooking(feature_matrix,k=3):\n",
        "    dist_matrix = pairwise_distances(feature_matrix)   # Pairwise distance of the samples\n",
        "    \n",
        "    min_dist_arr = np.zeros(dist_matrix.shape[0] * k)\n",
        "\n",
        "    ## Find the 3 nearest distance for each of the samples\n",
        "    for i in range(dist_matrix.shape[0]):\n",
        "        nearest_k = np.sort(dist_matrix[i,:])[1:(1+k)]\n",
        "        min_dist_arr[i:(i+k)] = nearest_k\n",
        "\n",
        "\n",
        "    min_dist_arr = np.sort(min_dist_arr)\n",
        "    print(min_dist_arr)\n",
        "\n",
        "    ## Finding the maximum slope of the distance, and return this value as the optimal epsilon\n",
        "    eps = max([x - z for x, z in zip(min_dist_arr[:-1], min_dist_arr[1:])])\n",
        "\n",
        "    return (eps, min_dist_arr)  # The dist_arr can be used to visualize the point,\n",
        "    # return max_slope            # For compactness, use this return statement instead of the one before it\n",
        "\n",
        "\n",
        "### Small test for the function\n",
        "results = best_eps_cooking(tfidf_matrix,3)\n",
        "eps = results[0]\n",
        "dist_sorted = results[1]\n",
        "\n",
        "# sns.lineplot(data=dist_sorted).set(title=\"Best epsilon: {:.3f}\".format(eps))\n",
        "eps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Apply dbscan to the generated matrix\n",
        "dbs = DBSCAN(eps=best_eps_cooking(tfidf_matrix)[0], min_samples=10, metric='cosine')\n",
        "dbs.fit(tfidf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Create a dataframe that only contains the url, the category and the labels\n",
        "\n",
        "df['label'] = dbs.labels_\n",
        "\n",
        "clustered = df[['url', 'category', 'label']]\n",
        "clustered.head()\n",
        "\n",
        "clustered.groupby('label').size()\n",
        "\n",
        "dist = pairwise_distances()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SRjiZhvKtcUZ",
        "outputId": "878c7a03-765b-461e-d9bc-dd1b8b1272d8"
      },
      "outputs": [],
      "source": [
        "# # Define a pipeline combining a text feature extractor with a simple clusterer\n",
        "\n",
        "# nCluster = 10\n",
        "\n",
        "# ## Maybe run the k-means for some k times\n",
        "# pipe = Pipeline(steps=\n",
        "#     [\n",
        "#         (\"tfidfVec\", TfidfVectorizer(max_df=0.5, min_df=5, stop_words=\"english\")),\n",
        "#         (\"dbs\", DBSCAN(eps=0.7, min_samples=4)),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# pipe.fit(data)\n",
        "# # km = pipe['km']\n",
        "# df['label'] = pipe['dbs'].labels_\n",
        "# clustered = df[['title', 'label', 'category']].sort_values(['label', 'category'])\n",
        "# clustered['category'] = clustered['category'].replace({'Culture':'C', 'Emerging Tech':'ET', 'Events':'E', 'Innovation & Design':'I&D', 'Learning':'L', 'News':'N', 'Opinion':'O', 'Product':'P', 'Strategy':'S', 'Technology':'T', 'Ways of Working': 'WW'})\n",
        "# clustered.head()\n",
        "# # silhouette_score(X=pipe.transform(clustered['title']), labels = clustered['label'])\n",
        "\n",
        "\n",
        "\n",
        "# # clustered['category'].drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plotting\n",
        "\n",
        "The figure below is the histograms of the categories in each cluster label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBZtZ0DWt7VZ"
      },
      "outputs": [],
      "source": [
        "# plt.rcParams['figure.figsize'] = (20, 12)\n",
        "# figure, axis = plt.subplots(5, 2, sharex=True)\n",
        "\n",
        "# for i in range(10):\n",
        "#     axis[i//2, i%2].hist(clustered[clustered['label'] == i]['category'], ec='black')\n",
        "#     axis[i//2, i%2].set_title('Label = {}'.format(i))\n",
        "\n",
        "# # plt.hist(clustered[clustered['label'] == 0]['category'], ec='black')\n",
        "\n",
        "\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "# categories = ['C', 'L', 'N', 'WW', 'ET', 'E', 'I&D', 'O', 'S', 'T', 'P']\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Correlation between cluster labels and original categories\n",
        "The statistic of interest is Cramer's V. This statistic takes into account the number of times that a label and a category are observed together. Since the statistic is usually optimistic, I used a bias corrected version of the statistic from Wikipedia.\n",
        "\n",
        "The following notations are used:\n",
        "\n",
        "-   $n$ is the number of blogs\n",
        "-   $n_{ij}$ is the number of times that a blog of categories $i$ is clustered into cluster $j$\n",
        "-   $n_i$ is the number of blogs in category $i$\n",
        "-   $n_j$ is the number of number of blogs in cluster $j$\n",
        "-   $r$ is the number of categories\n",
        "-   $k$ is the number of clusters\n",
        "-   $\\chi^2$ is the chi-squared statistic:\n",
        "    $$\\chi^2 = \\sum_{i, j}{\\frac{(n_{ij} - \\frac{n_i \\cdot n_j}{n})^2}{\\frac{n_i \\cdot n_j}{n}}}$$\n",
        "\n",
        "In addition, let:\n",
        "\n",
        "-   $\\tilde{\\varphi} = \\min{(0, \\frac{\\chi^2}{n} - \\frac{(k-1)(r-1)}{n-1})}$\n",
        "\n",
        "-   $\\tilde{r} = r - \\frac{(r-1)^2}{n-1}$\n",
        "\n",
        "-   $\\tilde{k} = k - \\frac{(k-1)^2}{n-1}$\n",
        "\n",
        "Thus, the formula for this statistic is:\n",
        "\n",
        "$$\\tilde{V} = \\sqrt{\\frac{\\tilde{\\varphi}^2}{\\min{(\\tilde{k}-1, \\tilde{n}-1)}}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def cramerV(frame, k, category_List):\n",
        "#     freq_count = frame.set_index([\"label\", \"category\"]).sort_index()  \n",
        "#     freq_count = freq_count.groupby(level=[0,1]).size().unstack().fillna(0).stack()\n",
        "#     freq_label = freq_count.sum(level=0)      # Group by labels (i) -> Sum over categories (j)\n",
        "#     freq_category = freq_count.sum(level=1)   # Group by categories (j) -> Sum over labels (i)\n",
        "#     n = len(frame)\n",
        "#     r = len(category_List)\n",
        "#     chi_squared = 0.0\n",
        "\n",
        "#     for label in range(nCluster):    # i\n",
        "#         for category in category_List:  # j\n",
        "#             n_i = freq_label[label]         # Sum over j\n",
        "#             n_j = freq_category[category]   # Sum over i\n",
        "#             n_ij = freq_count[label][category]\n",
        "            \n",
        "#             # Calculate the statistic to add\n",
        "#             denom = (n_i * n_j)/n\n",
        "#             statistic = ( (n_ij - denom)**2 ) / denom\n",
        "#             chi_squared += statistic\n",
        "\n",
        "#     corrected_coef = (k-1)*(r-1)/(n-1)\n",
        "#     corrected_chi_squared = max(0, chi_squared/n - corrected_coef)\n",
        "\n",
        "#     k_tilde = k - (k-1)**2/(n-1)\n",
        "#     r_tilde = r - (r-1)**2/(n-1)\n",
        "#     return np.sqrt(corrected_chi_squared / min(k_tilde-1, r_tilde - 1))\n",
        "\n",
        "# cramerV(clustered, nCluster, categories)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
