{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SRu9RziftcUX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth, KMeans, DBSCAN\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RI31bbDXtcUY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 814 entries, 0 to 813\n",
            "Data columns (total 10 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   url               814 non-null    object \n",
            " 1   title             814 non-null    object \n",
            " 2   time              786 non-null    object \n",
            " 3   category          782 non-null    object \n",
            " 4   text              805 non-null    object \n",
            " 5   pageviews         814 non-null    int64  \n",
            " 6   unique_pageviews  814 non-null    int64  \n",
            " 7   avg_time          814 non-null    float64\n",
            " 8   bounce_rate       814 non-null    float64\n",
            " 9   exit%             814 non-null    float64\n",
            "dtypes: float64(3), int64(2), object(5)\n",
            "memory usage: 63.7+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "### Import the data\n",
        "\n",
        "df = pd.read_csv('../../data/final/futurice_blog_data.csv', delimiter='\\t')\n",
        "print(df.info())\n",
        "\n",
        "### Drop the rows that have NaN title:\n",
        "df.dropna(subset=['title'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uq8X6QpqtcUZ"
      },
      "outputs": [],
      "source": [
        "data = np.array(df['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Use for tokenize in the tf-idf. Taken from http://brandonrose.org/clustering#Visualizing-document-clusters\n",
        "\n",
        "# Stemmer from nltk snowball stemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Using tfidf_Vectorizer to calculate the tfidf matrix\n",
        "# tfidf_vectorizer = TfidfVectorizer(max_df=0.5,\n",
        "                                #  min_df=5, stop_words='english',\n",
        "                                #  use_idf=True, tokenizer=tokenize_and_stem)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=5, stop_words=\"english\")\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 0. ... 1. 1. 1.]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Function for finding the best epsilon\n",
        "## The feature matrix should be in the form of row matrix, meaning that each sample is in one row of the matrix\n",
        "## k is the number of nearest point that the function will consider\n",
        "\n",
        "\n",
        "def best_eps_cooking(feature_matrix,k=3):\n",
        "    dist_matrix = pairwise_distances(feature_matrix)   # Pairwise distance of the samples\n",
        "    \n",
        "    min_dist_arr = np.zeros(dist_matrix.shape[0] * k)\n",
        "\n",
        "    ## Find the 3 nearest distance for each of the samples\n",
        "    for i in range(dist_matrix.shape[0]):\n",
        "        nearest_k = np.sort(dist_matrix[i,:])[1:(1+k)]\n",
        "        min_dist_arr[i:(i+k)] = nearest_k\n",
        "\n",
        "\n",
        "    min_dist_arr = np.sort(min_dist_arr)\n",
        "    print(min_dist_arr)\n",
        "\n",
        "    ## Finding the maximum slope of the distance, and return this value as the optimal epsilon\n",
        "    eps = max([x - z for x, z in zip(min_dist_arr[:-1], min_dist_arr[1:])])\n",
        "\n",
        "    return (eps, min_dist_arr)  # The dist_arr can be used to visualize the point,\n",
        "    # return max_slope            # For compactness, use this return statement instead of the one before it\n",
        "\n",
        "\n",
        "### Small test for the function\n",
        "results = best_eps_cooking(tfidf_matrix,3)\n",
        "eps = results[0]\n",
        "dist_sorted = results[1]\n",
        "\n",
        "# sns.lineplot(data=dist_sorted).set(title=\"Best epsilon: {:.3f}\".format(eps))\n",
        "eps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 0. ... 1. 1. 1.]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "eps == 0.0, must be > 0.0.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\dng09\\Desktop\\Project\\Futurice-blog-analysis\\models\\experimenting_with_titles.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dng09/Desktop/Project/Futurice-blog-analysis/models/experimenting_with_titles.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## Apply dbscan to the generated matrix\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dng09/Desktop/Project/Futurice-blog-analysis/models/experimenting_with_titles.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dbs \u001b[39m=\u001b[39m DBSCAN(eps\u001b[39m=\u001b[39mbest_eps_cooking(tfidf_matrix)[\u001b[39m0\u001b[39m], min_samples\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dng09/Desktop/Project/Futurice-blog-analysis/models/experimenting_with_titles.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dbs\u001b[39m.\u001b[39;49mfit(tfidf_matrix)\n",
            "File \u001b[1;32mc:\\Users\\dng09\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_dbscan.py:363\u001b[0m, in \u001b[0;36mDBSCAN.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    360\u001b[0m         X\u001b[39m.\u001b[39msetdiag(X\u001b[39m.\u001b[39mdiagonal())  \u001b[39m# XXX: modifies X's internals in-place\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[39m# Validating the scalar parameters.\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m check_scalar(\n\u001b[0;32m    364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    365\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    366\u001b[0m     target_type\u001b[39m=\u001b[39;49mnumbers\u001b[39m.\u001b[39;49mReal,\n\u001b[0;32m    367\u001b[0m     min_val\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[0;32m    368\u001b[0m     include_boundaries\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mneither\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    369\u001b[0m )\n\u001b[0;32m    370\u001b[0m check_scalar(\n\u001b[0;32m    371\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_samples,\n\u001b[0;32m    372\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmin_samples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m     include_boundaries\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    376\u001b[0m )\n\u001b[0;32m    377\u001b[0m check_scalar(\n\u001b[0;32m    378\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleaf_size,\n\u001b[0;32m    379\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mleaf_size\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     include_boundaries\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    383\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\dng09\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1480\u001b[0m, in \u001b[0;36mcheck_scalar\u001b[1;34m(x, name, target_type, min_val, max_val, include_boundaries)\u001b[0m\n\u001b[0;32m   1476\u001b[0m comparison_operator \u001b[39m=\u001b[39m (\n\u001b[0;32m   1477\u001b[0m     operator\u001b[39m.\u001b[39mlt \u001b[39mif\u001b[39;00m include_boundaries \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mboth\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m operator\u001b[39m.\u001b[39mle\n\u001b[0;32m   1478\u001b[0m )\n\u001b[0;32m   1479\u001b[0m \u001b[39mif\u001b[39;00m min_val \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m comparison_operator(x, min_val):\n\u001b[1;32m-> 1480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1481\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m == \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m, must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1482\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m>=\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m include_boundaries \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mboth\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mmin_val\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1483\u001b[0m     )\n\u001b[0;32m   1485\u001b[0m comparison_operator \u001b[39m=\u001b[39m (\n\u001b[0;32m   1486\u001b[0m     operator\u001b[39m.\u001b[39mgt \u001b[39mif\u001b[39;00m include_boundaries \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mright\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mboth\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m operator\u001b[39m.\u001b[39mge\n\u001b[0;32m   1487\u001b[0m )\n\u001b[0;32m   1488\u001b[0m \u001b[39mif\u001b[39;00m max_val \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m comparison_operator(x, max_val):\n",
            "\u001b[1;31mValueError\u001b[0m: eps == 0.0, must be > 0.0."
          ]
        }
      ],
      "source": [
        "## Apply dbscan to the generated matrix\n",
        "dbs = DBSCAN(eps=best_eps_cooking(tfidf_matrix)[0], min_samples=10, metric='cosine')\n",
        "dbs.fit(tfidf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Create a dataframe that only contains the url, the category and the labels\n",
        "\n",
        "df['label'] = dbs.labels_\n",
        "\n",
        "clustered = df[['url', 'category', 'label']]\n",
        "clustered.head()\n",
        "\n",
        "clustered.groupby('label').size()\n",
        "\n",
        "dist = pairwise_distances()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SRjiZhvKtcUZ",
        "outputId": "878c7a03-765b-461e-d9bc-dd1b8b1272d8"
      },
      "outputs": [],
      "source": [
        "# # Define a pipeline combining a text feature extractor with a simple clusterer\n",
        "\n",
        "# nCluster = 10\n",
        "\n",
        "# ## Maybe run the k-means for some k times\n",
        "# pipe = Pipeline(steps=\n",
        "#     [\n",
        "#         (\"tfidfVec\", TfidfVectorizer(max_df=0.5, min_df=5, stop_words=\"english\")),\n",
        "#         (\"dbs\", DBSCAN(eps=0.7, min_samples=4)),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# pipe.fit(data)\n",
        "# # km = pipe['km']\n",
        "# df['label'] = pipe['dbs'].labels_\n",
        "# clustered = df[['title', 'label', 'category']].sort_values(['label', 'category'])\n",
        "# clustered['category'] = clustered['category'].replace({'Culture':'C', 'Emerging Tech':'ET', 'Events':'E', 'Innovation & Design':'I&D', 'Learning':'L', 'News':'N', 'Opinion':'O', 'Product':'P', 'Strategy':'S', 'Technology':'T', 'Ways of Working': 'WW'})\n",
        "# clustered.head()\n",
        "# # silhouette_score(X=pipe.transform(clustered['title']), labels = clustered['label'])\n",
        "\n",
        "\n",
        "\n",
        "# # clustered['category'].drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plotting\n",
        "\n",
        "The figure below is the histograms of the categories in each cluster label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBZtZ0DWt7VZ"
      },
      "outputs": [],
      "source": [
        "# plt.rcParams['figure.figsize'] = (20, 12)\n",
        "# figure, axis = plt.subplots(5, 2, sharex=True)\n",
        "\n",
        "# for i in range(10):\n",
        "#     axis[i//2, i%2].hist(clustered[clustered['label'] == i]['category'], ec='black')\n",
        "#     axis[i//2, i%2].set_title('Label = {}'.format(i))\n",
        "\n",
        "# # plt.hist(clustered[clustered['label'] == 0]['category'], ec='black')\n",
        "\n",
        "\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "# categories = ['C', 'L', 'N', 'WW', 'ET', 'E', 'I&D', 'O', 'S', 'T', 'P']\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Correlation between cluster labels and original categories\n",
        "The statistic of interest is Cramer's V. This statistic takes into account the number of times that a label and a category are observed together. Since the statistic is usually optimistic, I used a bias corrected version of the statistic from Wikipedia.\n",
        "\n",
        "The following notations are used:\n",
        "\n",
        "-   $n$ is the number of blogs\n",
        "-   $n_{ij}$ is the number of times that a blog of categories $i$ is clustered into cluster $j$\n",
        "-   $n_i$ is the number of blogs in category $i$\n",
        "-   $n_j$ is the number of number of blogs in cluster $j$\n",
        "-   $r$ is the number of categories\n",
        "-   $k$ is the number of clusters\n",
        "-   $\\chi^2$ is the chi-squared statistic:\n",
        "    $$\\chi^2 = \\sum_{i, j}{\\frac{(n_{ij} - \\frac{n_i \\cdot n_j}{n})^2}{\\frac{n_i \\cdot n_j}{n}}}$$\n",
        "\n",
        "In addition, let:\n",
        "\n",
        "-   $\\tilde{\\varphi} = \\min{(0, \\frac{\\chi^2}{n} - \\frac{(k-1)(r-1)}{n-1})}$\n",
        "\n",
        "-   $\\tilde{r} = r - \\frac{(r-1)^2}{n-1}$\n",
        "\n",
        "-   $\\tilde{k} = k - \\frac{(k-1)^2}{n-1}$\n",
        "\n",
        "Thus, the formula for this statistic is:\n",
        "\n",
        "$$\\tilde{V} = \\sqrt{\\frac{\\tilde{\\varphi}^2}{\\min{(\\tilde{k}-1, \\tilde{n}-1)}}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def cramerV(frame, k, category_List):\n",
        "#     freq_count = frame.set_index([\"label\", \"category\"]).sort_index()  \n",
        "#     freq_count = freq_count.groupby(level=[0,1]).size().unstack().fillna(0).stack()\n",
        "#     freq_label = freq_count.sum(level=0)      # Group by labels (i) -> Sum over categories (j)\n",
        "#     freq_category = freq_count.sum(level=1)   # Group by categories (j) -> Sum over labels (i)\n",
        "#     n = len(frame)\n",
        "#     r = len(category_List)\n",
        "#     chi_squared = 0.0\n",
        "\n",
        "#     for label in range(nCluster):    # i\n",
        "#         for category in category_List:  # j\n",
        "#             n_i = freq_label[label]         # Sum over j\n",
        "#             n_j = freq_category[category]   # Sum over i\n",
        "#             n_ij = freq_count[label][category]\n",
        "            \n",
        "#             # Calculate the statistic to add\n",
        "#             denom = (n_i * n_j)/n\n",
        "#             statistic = ( (n_ij - denom)**2 ) / denom\n",
        "#             chi_squared += statistic\n",
        "\n",
        "#     corrected_coef = (k-1)*(r-1)/(n-1)\n",
        "#     corrected_chi_squared = max(0, chi_squared/n - corrected_coef)\n",
        "\n",
        "#     k_tilde = k - (k-1)**2/(n-1)\n",
        "#     r_tilde = r - (r-1)**2/(n-1)\n",
        "#     return np.sqrt(corrected_chi_squared / min(k_tilde-1, r_tilde - 1))\n",
        "\n",
        "# cramerV(clustered, nCluster, categories)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
