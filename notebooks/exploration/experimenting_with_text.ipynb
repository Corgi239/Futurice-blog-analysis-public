{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth, KMeans, DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the data\n",
    "\n",
    "df = pd.read_csv('../../data/final/futurice_blog_data.csv', delimiter='\\t')\n",
    "print(df.info())\n",
    "\n",
    "### Drop the rows that have NaN text:\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "# data = df['text']\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use for tokenize in the tf-idf. Taken from http://brandonrose.org/clustering#Visualizing-document-clusters\n",
    "\n",
    "# Stemmer from nltk snowball stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using tfidf_Vectorizer to calculate the tfidf matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem)\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for finding the best epsilon\n",
    "## The feature matrix should be in the form of row matrix, meaning that each sample is in one row of the matrix\n",
    "## k is the number of nearest point that the function will consider\n",
    "\n",
    "\n",
    "def best_eps_cooking(feature_matrix,k=3):\n",
    "    dist_matrix = pairwise_distances(feature_matrix)   # Pairwise distance of the samples\n",
    "    \n",
    "    min_dist_arr = np.zeros(dist_matrix.shape[0] * k)\n",
    "\n",
    "    ## Find the 3 nearest distance for each of the samples\n",
    "    for i in range(dist_matrix.shape[0]):\n",
    "        nearest_k = np.sort(dist_matrix[i,:])[1:(1+k)]\n",
    "        min_dist_arr[i:(i+k)] = nearest_k\n",
    "\n",
    "\n",
    "    ## Finding the maximum slope of the distance, and return this value as the optimal epsilon\n",
    "    eps = max([x - z for x, z in zip(min_dist_arr[:-1], min_dist_arr[1:])])\n",
    "\n",
    "    return (eps, min_dist_arr)  # The dist_arr can be used to visualize the point,\n",
    "    # return max_slope            # For compactness, use this return statement instead of the one before it\n",
    "\n",
    "\n",
    "### Small test for the function\n",
    "results = best_eps_cooking(tfidf_matrix, 20)\n",
    "eps = results[0]\n",
    "dist_sorted = np.sort(results[1])\n",
    "\n",
    "sns.lineplot(data=dist_sorted).set(title=\"Best epsilon: {:.3f}\".format(eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply dbscan to the generated matrix\n",
    "dbs = DBSCAN(eps=best_eps_cooking(tfidf_matrix)[0], min_samples=10, metric='cosine')\n",
    "dbs.fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a dataframe that only contains the url, the category and the labels\n",
    "\n",
    "df['label'] = dbs.labels_\n",
    "\n",
    "clustered = df[['url', 'category', 'label']]\n",
    "clustered.head()\n",
    "\n",
    "clustered.groupby('label').size()\n",
    "\n",
    "dist = pairwise_distances()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
