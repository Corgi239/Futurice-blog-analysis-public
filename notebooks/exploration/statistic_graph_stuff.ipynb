{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from raceplotly.plots import barplot\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## First time users should uncomment the below two lines\n",
    "    \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the data is read into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the data\n",
    "\n",
    "df = pd.read_csv(\"../../data/final/futurice_blog_data.csv\", sep=\"\\t\")\n",
    "# df\n",
    "df.dropna(subset=[\"text\"], inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs of the basic statistics\n",
    "## Average sentence length, with the problematics results removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[df[\"average_sentence_length\"] < 300 ][\"average_sentence_length\"], bins=20)\n",
    "plt.xlabel('Sentence length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Average text length histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[df[\"average_sentence_length\"] < 300 ][\"flesch\"], bins=20)\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Flesch reading ease scores histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[df[\"average_sentence_length\"] < 300 ][\"dale_chall\"], bins=20)\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Dale-Chall readability score histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[df[\"average_sentence_length\"] < 300 ][\"text_length\"], bins=20)\n",
    "plt.xlabel('Sentence length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Total text length histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some additional statistics\n",
    "These are statistics that might not be able to fit into the csv file\n",
    "\n",
    "## Most common word in a period of time\n",
    "In this part, there are some rows that the date is `nan`. For those rows, I just remove them completely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can start doing the real work. But first, let's try to split the data into different month interval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions \n",
    "# Generate interval based on a date range\n",
    "def get_date_interval(startDate, endDate, month_interval):\n",
    "    s = pd.date_range(start=startDate, end=endDate, freq=str(month_interval)+\"MS\", inclusive='left')\n",
    "    e = (s[1:]-pd.to_timedelta(1, unit='D'))\n",
    "    return list(zip(s.strftime('%Y-%m-%d').tolist(), e.strftime('%Y-%m-%d').tolist() + [endDate]))\n",
    "\n",
    "# Preprocessing: tokenization, stopwords removal, lemmatization, and stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def lemmatize_stem(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text, min_len=3):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stem(token))\n",
    "    return result\n",
    "\n",
    "\n",
    "def cooking_2(df_main, period=1):\n",
    "    ## Preprocessing and generating the lists that we will need\n",
    "    df_ = df_main.copy()\n",
    "    df_[\"time\"] = pd.to_datetime(df_[\"time\"])\n",
    "    df_ = df_[df_[\"time\"].notna()]   # Filling the nan rows of the time in dataframe\n",
    "    df_ = df_.sort_values(by='time',ascending=True)  # Sort the rows by the date\n",
    "    \n",
    "    # Get all available date interval\n",
    "    date_intervals = get_date_interval(df_[\"time\"].iloc[0], df_[\"time\"].iloc[-1], period)\n",
    "    date_intervals[-1] = (date_intervals[-1][0], date_intervals[-1][1].strftime('%Y-%m-%d'))\n",
    "\n",
    "    ## The main loop:\n",
    "    blog_in_range = {}   # The result list\n",
    "    # Since the helper function does not generate the interval starting from the middle of the month, I have to compromise by manually adding them into the temp list\n",
    "    temp_list = [df_.iloc[0][\"text\"], df_.iloc[1][\"text\"]] \n",
    "    index = 0\n",
    "    i=2   # Thus, we are starting from index 2\n",
    "    total_blogs = 2\n",
    "\n",
    "    while i < len(df_[\"time\"]):\n",
    "        current_row = df_.iloc[i]   # Get the current row of the dataframe\n",
    "        current_interval = date_intervals[index]  # What is the current interval that we are considering?\n",
    "\n",
    "        # If the date is in the current interval, add the row to the temp list\n",
    "        if(datetime.strptime(current_interval[0], '%Y-%m-%d') <= current_row[\"time\"] <= datetime.strptime(current_interval[1], '%Y-%m-%d')):  \n",
    "            temp_list.append(current_row[\"text\"])\n",
    "            total_blogs += 1\n",
    "            i += 1\n",
    "    \n",
    "        # If the date is not in the interval, it means that we are going to the next interval, adding the temp list into the result and increment the index\n",
    "        else:             \n",
    "            blog_in_range[current_interval] = (temp_list, total_blogs)\n",
    "            temp_list = []\n",
    "            total_blogs = 0\n",
    "            index += 1\n",
    "    blog_in_range[date_intervals[-1]] = (temp_list, total_blogs)\n",
    "\n",
    "    ## Final processing to get the results\n",
    "    blog_in_range = dict(filter(lambda pair: len(pair[1][0]) != 0, blog_in_range.items()))         # Remove the intervals that do not contain any texts\n",
    "    blog_in_range = { interval[1]:(Counter(preprocess(\" \".join(blogs))), count) for (interval, (blogs, count)) in blog_in_range.items() } # Combined all the text in the intervals\n",
    "    for (interval, (counter, total)) in blog_in_range.items():\n",
    "        for item, count in counter.items():\n",
    "            counter[item] /= total\n",
    "        blog_in_range[interval] = counter\n",
    "    return blog_in_range\n",
    "\n",
    "test = cooking_2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word trend throughout the months\n",
    "\n",
    "Here, I create a new function to return a trend graph automatically. The function requires the dataframe, the period length (which is default to 1), the number of bar the users want to appear each time (which is default to 7), and the speed of the graph (defaulted to 500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffing_rpl_2(df, period=1, nbars=10, plength=1000):\n",
    "    _trend = pd.DataFrame(cooking_2(df, period)).fillna(0).T\n",
    "    _trend = _trend.melt(ignore_index=False).reset_index().rename(columns={\"index\":\"date\"})\n",
    "    myplot = barplot(_trend, item_column=\"variable\", value_column=\"value\", time_column=\"date\", top_entries=nbars)\n",
    "    fig = myplot.plot(title=\"Word popularity by {:d}-month period\".format(period), item_label=\"Words\", value_label=\"Count\", time_label=\"Time: \", frame_duration=plength)\n",
    "    fig.update_layout(\n",
    "                font={'size':17},\n",
    "                plot_bgcolor='black',\n",
    "                height=600\n",
    "                )\n",
    "    fig.write_html(\"../data/figs/{:d}months.html\".format(period))\n",
    "    return\n",
    "\n",
    "# huffing_rpl_2(df, period=1)\n",
    "# huffing_rpl_2(df, period=3)\n",
    "# huffing_rpl_2(df, period=6)\n",
    "# huffing_rpl_2(df, period=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format \n",
    "`{ date : ([(word, count)], total_text) }`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_in_range(df_main, period=1):\n",
    "     ## Preprocessing and generating the lists that we will need\n",
    "    df_ = df_main.copy()\n",
    "    df_[\"time\"] = pd.to_datetime(df_[\"time\"])\n",
    "    df_ = df_[df_[\"time\"].notna()]   # Filling the nan rows of the time in dataframe\n",
    "    df_ = df_.sort_values(by='time',ascending=True)  # Sort the rows by the date\n",
    "    \n",
    "    # Get all available date interval\n",
    "    date_intervals = get_date_interval(df_[\"time\"].iloc[0], df_[\"time\"].iloc[-1], period)\n",
    "    date_intervals[-1] = (date_intervals[-1][0], date_intervals[-1][1].strftime('%Y-%m-%d'))\n",
    "\n",
    "    ## The main loop:\n",
    "    blog_in_range = {}   # The result list\n",
    "    # Since the helper function does not generate the interval starting from the middle of the month, I have to compromise by manually adding them into the temp list\n",
    "    temp_list = [df_.iloc[0][\"text\"], df_.iloc[1][\"text\"]] \n",
    "    index = 0\n",
    "    i=2   # Thus, we are starting from index 2\n",
    "    total_blogs = 2\n",
    "\n",
    "    while i < len(df_[\"time\"]):\n",
    "        current_row = df_.iloc[i]   # Get the current row of the dataframe\n",
    "        current_interval = date_intervals[index]  # What is the current interval that we are considering?\n",
    "\n",
    "        # If the date is in the current interval, add the row to the temp list\n",
    "        if(datetime.strptime(current_interval[0], '%Y-%m-%d') <= current_row[\"time\"] <= datetime.strptime(current_interval[1], '%Y-%m-%d')):  \n",
    "            temp_list.append(current_row[\"text\"])\n",
    "            total_blogs += 1\n",
    "            i += 1\n",
    "    \n",
    "        # If the date is not in the interval, it means that we are going to the next interval, adding the temp list into the result and increment the index\n",
    "        else:             \n",
    "            blog_in_range[current_interval] = (temp_list, total_blogs)\n",
    "            temp_list = []\n",
    "            total_blogs = 0\n",
    "            index += 1\n",
    "    blog_in_range[date_intervals[-1]] = (temp_list, total_blogs)\n",
    "    blog_in_range = dict(filter(lambda pair: len(pair[1][0]) != 0, blog_in_range.items()))         # Remove the intervals that do not contain any texts\n",
    "    return { interval[1]:texts for (interval, texts) in blog_in_range.items() }\n",
    "    \n",
    "\n",
    "def grillin(df_main, period=1, ngram_range=(2, 2)):\n",
    "    blog_in_range = get_blog_in_range(df_main, period)\n",
    "    for (interval, (text_list, total_text)) in blog_in_range.items():\n",
    "        combined_text = \" \".join(text_list)\n",
    "        vectorizer = CountVectorizer(stop_words='english', ngram_range=ngram_range)\n",
    "        matrix = vectorizer.fit_transform([combined_text])\n",
    "        token_with_count = list(zip(vectorizer.get_feature_names_out(), matrix.toarray()[0]))\n",
    "        token_with_count = list(filter(lambda pair: not pair[0].isnumeric(), token_with_count))\n",
    "        blog_in_range[interval] = { word:counts/total_text for (word, counts) in token_with_count }\n",
    "    return blog_in_range\n",
    "\n",
    "# test2 = grillin(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2= pd.DataFrame(test2).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffing_rpl_3(df, period=1, nbars=10, plength=1000):\n",
    "    _trend = pd.DataFrame(grillin(df, period)).fillna(0).T\n",
    "    _trend = _trend.melt(ignore_index=False, value_name='frequency_count').reset_index().rename(columns={\"index\":\"date\"})\n",
    "    myplot = barplot(_trend, item_column=\"variable\", value_column=\"frequency_count\", time_column=\"date\", top_entries=nbars)\n",
    "    fig = myplot.plot(title=\"Word popularity by {:d}-month period\".format(period), item_label=\"Words\", value_label=\"Count\", time_label=\"Time: \", frame_duration=plength)\n",
    "    fig.update_layout(\n",
    "                font={'size':17},\n",
    "                plot_bgcolor='black',\n",
    "                height=600\n",
    "                )\n",
    "    fig.write_html(\"../data/figs/{:d}months_bigrams.html\".format(period))\n",
    "    return\n",
    "\n",
    "# huffing_rpl_3(df, period=1)\n",
    "# huffing_rpl_3(df, period=3)\n",
    "# huffing_rpl_3(df, period=6)\n",
    "# huffing_rpl_3(df, period=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(stop_words='english', ngram_range=(2, 2))\n",
    "# matrix = vectorizer.fit_transform(['technology revolution technological revolutionise'])\n",
    "# list(zip(vectorizer.get_feature_names_out(), matrix.toarray()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
